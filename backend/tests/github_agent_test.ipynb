{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from typing import Dict, List, cast, Optional, Any\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated, Sequence\n",
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    session_id: str = \"\"\n",
    "    github_url: str = \"\"\n",
    "    company_name: str = \"\"\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages] = field(default_factory=list)\n",
    "\n",
    "# ì„¸ì…˜ ì´ˆê¸°í™” (ìŠ¤í¬ë˜í•‘ìš©)\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "})\n",
    "\n",
    "def _scrape_page(url: str) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"ì›¹ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            return BeautifulSoup(r.text, 'html.parser')\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "@tool\n",
    "def scrape_user_profile(username: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape GitHub user profile information from their main page\n",
    "    \n",
    "    Args:\n",
    "        username: GitHub username\n",
    "    \n",
    "    Returns:\n",
    "        ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    soup = _scrape_page(f\"https://github.com/{username}\")\n",
    "    if not soup:\n",
    "        return {\"error\": \"Failed to scrape profile\"}\n",
    "\n",
    "    result = {\n",
    "        \"username\": username, \"name\": None, \"bio\": None, \"location\": None,\n",
    "        \"company\": None, \"followers\": None, \"following\": None, \"repositories\": None\n",
    "    }\n",
    "\n",
    "    name_elem = soup.select_one('.vcard-fullname')\n",
    "    if name_elem: result[\"name\"] = name_elem.text.strip()\n",
    "\n",
    "    bio_elem = soup.select_one('.user-profile-bio > div')\n",
    "    if bio_elem: result[\"bio\"] = bio_elem.text.strip()\n",
    "\n",
    "    for item in soup.select('.vcard-detail'):\n",
    "        text = item.text.strip()\n",
    "        if 'location' in str(item):\n",
    "            result[\"location\"] = text\n",
    "        elif 'organization' in str(item):\n",
    "            result[\"company\"] = text\n",
    "\n",
    "    for link in soup.select('a.Link--secondary'):\n",
    "        href = link.get('href', '')\n",
    "        text = link.text.strip()\n",
    "        if 'followers' in href:\n",
    "            result[\"followers\"] = text.split()[0]\n",
    "        elif 'following' in href:\n",
    "            result[\"following\"] = text.split()[0]\n",
    "\n",
    "    repo_count = soup.select_one('span.Counter')\n",
    "    if repo_count:\n",
    "        result[\"repositories\"] = repo_count.text.strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def scrape_user_repos(username: str, page: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape list of repositories from user's repositories tab\n",
    "    \n",
    "    Args:\n",
    "        username: GitHub username\n",
    "        page: Page number for pagination\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing list of repositories and count\n",
    "    \"\"\"\n",
    "    url = f\"https://github.com/{username}?tab=repositories\"\n",
    "    if page > 1: url += f\"&page={page}\"\n",
    "    soup = _scrape_page(url)\n",
    "    if not soup:\n",
    "        return {\"error\": \"Failed to scrape repositories\"}\n",
    "\n",
    "    repos = []\n",
    "    for repo_elem in soup.select('#user-repositories-list > ul > li'):\n",
    "        repo_data = {}\n",
    "        name_elem = repo_elem.select_one('a[itemprop=\"name codeRepository\"]')\n",
    "        if name_elem:\n",
    "            repo_data[\"name\"] = name_elem.text.strip()\n",
    "            repo_data[\"url\"] = f\"https://github.com{name_elem.get('href','')}\"\n",
    "        desc_elem = repo_elem.select_one('p[itemprop=\"description\"]')\n",
    "        if desc_elem: repo_data[\"description\"] = desc_elem.text.strip()\n",
    "        lang_elem = repo_elem.select_one('[itemprop=\"programmingLanguage\"]')\n",
    "        if lang_elem: repo_data[\"language\"] = lang_elem.text.strip()\n",
    "        star_elem = repo_elem.select_one('a[href*=\"stargazers\"]')\n",
    "        if star_elem: repo_data[\"stars\"] = star_elem.text.strip()\n",
    "        fork_elem = repo_elem.select_one('a[href*=\"forks\"]')\n",
    "        if fork_elem: repo_data[\"forks\"] = fork_elem.text.strip()\n",
    "        time_elem = repo_elem.select_one('relative-time')\n",
    "        if time_elem: repo_data[\"updated\"] = time_elem.get('datetime', '')\n",
    "        if repo_data.get(\"name\"): repos.append(repo_data)\n",
    "    return {\"repositories\": repos, \"count\": len(repos)}\n",
    "\n",
    "@tool\n",
    "def scrape_repo_main_page(owner: str, repo: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape repository main page for basic info and README\n",
    "    \n",
    "    Args:\n",
    "        owner: Repository owner username\n",
    "        repo: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing repository info, README, etc.\n",
    "    \"\"\"\n",
    "    soup = _scrape_page(f\"https://github.com/{owner}/{repo}\")\n",
    "    if not soup:\n",
    "        return {\"error\": \"Failed to scrape repository\"}\n",
    "\n",
    "    result = {\n",
    "        \"name\": repo, \"owner\": owner, \"description\": None, \"stars\": None,\n",
    "        \"forks\": None, \"watching\": None, \"readme\": None, \"languages\": {}, \"about\": {}\n",
    "    }\n",
    "\n",
    "    desc_elem = soup.select_one('p.f4')\n",
    "    if desc_elem: result[\"description\"] = desc_elem.text.strip()\n",
    "\n",
    "    for elem in soup.select('#repo-content-pjax-container a.Link--muted'):\n",
    "        href = elem.get('href',''); text = elem.text.strip()\n",
    "        if '/stargazers' in href: result[\"stars\"] = text.split()[0]\n",
    "        elif '/forks' in href: result[\"forks\"] = text.split()[0]\n",
    "        elif '/watchers' in href: result[\"watching\"] = text.split()[0]\n",
    "\n",
    "    readme_elem = soup.select_one('article.markdown-body')\n",
    "    if readme_elem:\n",
    "        readme_text = readme_elem.get_text(separator='\\n', strip=True)\n",
    "        result[\"readme\"] = readme_text[:3000]\n",
    "\n",
    "    for lang_elem in soup.select('.BorderGrid-cell span[itemprop=\"programmingLanguage\"]'):\n",
    "        lang = lang_elem.text.strip()\n",
    "        percent_elem = lang_elem.find_next('span', class_='ml-1')\n",
    "        if percent_elem:\n",
    "            result[\"languages\"][lang] = percent_elem.text.strip()\n",
    "\n",
    "    about_elem = soup.select_one('.BorderGrid')\n",
    "    if about_elem:\n",
    "        for link in about_elem.select('a'):\n",
    "            text = link.text.strip()\n",
    "            if 'release' in link.get('href',''):\n",
    "                result[\"about\"][\"releases\"] = text\n",
    "            elif 'commit' in link.get('href',''):\n",
    "                result[\"about\"][\"commits\"] = text\n",
    "            elif 'contributor' in link.get('href',''):\n",
    "                result[\"about\"][\"contributors\"] = text\n",
    "\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def scrape_repo_commits(owner: str, repo: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape recent commits from repository\n",
    "    \n",
    "    Args:\n",
    "        owner: Repository owner username\n",
    "        repo: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing list of commits and count\n",
    "    \"\"\"\n",
    "    soup = _scrape_page(f\"https://github.com/{owner}/{repo}/commits\")\n",
    "    if not soup:\n",
    "        return {\"error\": \"Failed to scrape commits\"}\n",
    "\n",
    "    commits = []\n",
    "    for commit_elem in soup.select('.Box-row')[:10]:\n",
    "        commit_data = {}\n",
    "        msg_elem = commit_elem.select_one('a.Link--primary')\n",
    "        if msg_elem: commit_data[\"message\"] = msg_elem.text.strip()\n",
    "        author_elem = commit_elem.select_one('a.commit-author')\n",
    "        if author_elem: commit_data[\"author\"] = author_elem.text.strip()\n",
    "        time_elem = commit_elem.select_one('relative-time')\n",
    "        if time_elem: commit_data[\"date\"] = time_elem.get('datetime','')\n",
    "        hash_elem = commit_elem.select_one('a.text-mono')\n",
    "        if hash_elem: commit_data[\"sha\"] = hash_elem.text.strip()\n",
    "        if commit_data: commits.append(commit_data)\n",
    "    return {\"commits\": commits, \"count\": len(commits)}\n",
    "\n",
    "@tool\n",
    "def scrape_repo_files(owner: str, repo: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape repository file structure\n",
    "    \n",
    "    Args:\n",
    "        owner: Repository owner username\n",
    "        repo: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing list of files and count\n",
    "    \"\"\"\n",
    "    soup = _scrape_page(f\"https://github.com/{owner}/{repo}\")\n",
    "    if not soup:\n",
    "        return {\"error\": \"Failed to scrape files\"}\n",
    "    files = []\n",
    "    for row in soup.select('div[role=\"row\"]'):\n",
    "        name_elem = row.select_one('a.Link--primary')\n",
    "        if name_elem:\n",
    "            name = name_elem.text.strip()\n",
    "            icon = row.select_one('svg')\n",
    "            file_type = \"file\"\n",
    "            if icon and 'directory' in str(icon):\n",
    "                file_type = \"directory\"\n",
    "            files.append({\"name\": name, \"type\": file_type})\n",
    "    return {\"files\": files, \"count\": len(files)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3d354b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scrape repository file structure\\n\\nArgs:\\n    owner: Repository owner username\\n    repo: Repository name\\n\\nReturns:\\n    Dictionary containing list of files and count'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_repo_files.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70f671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def github_agent(state: State) -> Dict[str, List[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Analyze GitHub profile and repository information\n",
    "    \"\"\"\n",
    "\n",
    "    AGENT_MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "    model = ChatOpenAI(model=AGENT_MODEL, temperature=0, api_key=OPENAI_API_KEY)\n",
    "\n",
    "    username = state.github_url.rstrip('/').split('/')[-1] if 'github.com/' in state.github_url else state.github_url\n",
    "    print(\"username: \", username)\n",
    "    # LangChain tool ë°ì½”ë ˆì´í„°ë¡œ ì •ì˜ëœ ë„êµ¬ë“¤ì„ ì‚¬ìš©\n",
    "    tools = [\n",
    "        scrape_user_profile,\n",
    "        scrape_user_repos,\n",
    "        scrape_repo_main_page,\n",
    "        scrape_repo_commits,\n",
    "        scrape_repo_files\n",
    "    ]\n",
    "\n",
    "    system_message = f\"\"\"You're an expert at analyzing GitHub repositories.\n",
    "    You use web scraping tools to analyze GitHub profiles and repositories.\n",
    "    \n",
    "    username: {username}\n",
    "\n",
    "    You are a GitHub repository analyst with web scraping tools.\n",
    "    Analyze GitHub profiles and repositories by scraping public web pages.\n",
    "    Always respond in Korean.\n",
    "    \n",
    "    Available scraping tools:\n",
    "    - scrape_user_profile: Get user profile information\n",
    "    - scrape_user_repos: Get list of repositories\n",
    "    - scrape_repo_main_page: Get repository details and README\n",
    "    - scrape_repo_commits: Get recent commits\n",
    "    - scrape_repo_files: Get file structure\n",
    "    \n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. Use tools strategically to gather information (maximum 3-4 tool calls)\n",
    "    2. After collecting sufficient information, provide a comprehensive Korean summary\n",
    "    3. Do NOT continue using tools indefinitely - stop when you have enough information\n",
    "    4. Your final response should be a complete analysis in Korean, not a request for more tools\n",
    "    \n",
    "    Process:\n",
    "    1. Start with scrape_user_profile or scrape_repo_main_page for basic info\n",
    "    2. Use 1-2 additional tools if needed for specific details\n",
    "    3. Provide final comprehensive summary in Korean and STOP\n",
    "    \"\"\"\n",
    "\n",
    "    # prompt = PromptTemplate.from_template(system_message)\n",
    "    agent = create_react_agent(model, tools, prompt=system_message)\n",
    "\n",
    "    # recursion_limit ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€\n",
    "    config = {\"recursion_limit\": 10, \"max_iterations\": 5}\n",
    "    response = await agent.ainvoke({\"messages\": state.messages}, config=config)\n",
    "    print(\"response: \", response)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response[\"messages\"][-1]],\n",
    "        \"agent_name\": \"github_agent\",\n",
    "        \"company_summary\": response[\"messages\"][-1].content,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45408496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤\n",
    "test_cases = [\n",
    "    {\n",
    "        \"description\": \"í…ŒìŠ¤íŠ¸ 1: ì‚¬ìš©ì í”„ë¡œí•„ ë¶„ì„\",\n",
    "        \"github_url\": \"https://github.com/Pseudo-Lab\",\n",
    "        \"query\": \"JobPT ë ˆí¬ì— ëŒ€í•´ì„œ ìš”ì•½í•´ì¤˜\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"í…ŒìŠ¤íŠ¸ 2: ì €ì¥ì†Œ ìƒì„¸ ë¶„ì„\",\n",
    "        \"github_url\": \"https://github.com/Pseudo-Lab/JobPT\",\n",
    "        \"query\": \"ì´ ì €ì¥ì†Œì˜ README, ìµœê·¼ ì»¤ë°‹, ì‚¬ìš© ì–¸ì–´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"í…ŒìŠ¤íŠ¸ 3: ì¡°ì§ ë¶„ì„\",\n",
    "        \"github_url\": \"https://github.com/Pseudo-Lab\",\n",
    "        \"query\": \"ì´ ì¡°ì§ì˜ ì£¼ìš” ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë“¤ì„ íŒŒì•…í•˜ê³ , ì–´ë–¤ ê¸°ìˆ  ìŠ¤íƒì„ ì‚¬ìš©í•˜ëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "state = State(\n",
    "    session_id=f\"test_session_1\",\n",
    "    github_url=test_cases[0]['github_url'],\n",
    "    company_name=test_cases[0]['github_url'].split('/')[-1],  # URLì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©\n",
    "    messages=[HumanMessage(content=test_cases[0]['query'])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username:  Pseudo-Lab\n",
      "response:  {'messages': [HumanMessage(content='JobPT ë ˆí¬ì— ëŒ€í•´ì„œ ìš”ì•½í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='5be3501a-aaab-435b-a90c-b9e1c540d2a9'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xK8vkoHOXQVWGIQbE1fTFUkJ', 'function': {'arguments': '{\"username\":\"Pseudo-Lab\"}', 'name': 'scrape_user_repos'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 566, 'total_tokens': 585, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CXnG7ZpDgqItQhdwsLU3Z3pxxCtfK', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--83b9ca82-bdcf-4e39-a6d8-32b2caa34041-0', tool_calls=[{'name': 'scrape_user_repos', 'args': {'username': 'Pseudo-Lab'}, 'id': 'call_xK8vkoHOXQVWGIQbE1fTFUkJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 566, 'output_tokens': 19, 'total_tokens': 585, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='{\"repositories\": [], \"count\": 0}', name='scrape_user_repos', id='3a435d6d-aae5-483b-aed0-4e58520c75a2', tool_call_id='call_xK8vkoHOXQVWGIQbE1fTFUkJ'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gn1FhSHsIkunzb8BvmbJPT8O', 'function': {'arguments': '{\"owner\":\"Pseudo-Lab\",\"repo\":\"JobPT\"}', 'name': 'scrape_repo_main_page'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 606, 'total_tokens': 630, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CXnGAXD4FCKdQLa2MfMcfWsyappCy', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b4a731f2-8380-4330-9468-8457fced8bda-0', tool_calls=[{'name': 'scrape_repo_main_page', 'args': {'owner': 'Pseudo-Lab', 'repo': 'JobPT'}, 'id': 'call_gn1FhSHsIkunzb8BvmbJPT8O', 'type': 'tool_call'}], usage_metadata={'input_tokens': 606, 'output_tokens': 24, 'total_tokens': 630, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='{\"name\": \"JobPT\", \"owner\": \"Pseudo-Lab\", \"description\": \"êµ¬ì§ìì˜ ì´ë ¥ì„œì™€ ì±„ìš© ê³µê³ ë¥¼ ë§¤ì¹­í•´ì£¼ê³ , ì±„ìš© ê³µê³ ì— ë§ëŠ” ì´ë ¥ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì£¼ëŠ” ì±„ìš©ê³µê³  ë§¤ì¹­ì‹œìŠ¤í…œ Resources\", \"stars\": \"32\", \"forks\": \"5\", \"watching\": \"0\", \"readme\": \"JobPT\\\\nì„œë¹„ìŠ¤ ê°œìš”\\\\ní˜„ëŒ€ ì‚¬íšŒì—ì„œ ì·¨ì—… ì¤€ë¹„ëŠ” ë°˜ë³µì ì´ê³  ì‹œê°„ ì†Œëª¨ì ì¸ ê³¼ì •ìœ¼ë¡œ, êµ¬ì§ìê°€ ìì‹ ì˜ ì—­ëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì–´í•„í•˜ì§€ ëª»í•´ ê¸°íšŒë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ JobPTëŠ” LLM ê¸°ë°˜\\\\nì±„ìš© ê³µê³  ì¶”ì²œ, ê¸°ì—… ë§ì¶¤í˜• ì´ë ¥ì„œ í”¼ë“œë°±, íšŒì‚¬ ì •ë³´ ìš”ì•½ ì œê³µ, ì´ë ¥ì„œ ì í•©ì„± í‰ê°€\\\\në“±ì˜ ê¸°ëŠ¥ì„ ê°–ì¶˜ \\'\\\\nì§€ëŠ¥í˜• ì·¨ì—… ì§€ì› ì„œë¹„ìŠ¤\\\\n\\'ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\\\nì£¼ìš” ê¸°ëŠ¥\\\\nLLM ê¸°ë°˜ ê°œì¸ ì´ë ¥ì„œ ë§ì¶¤í˜• ì±„ìš© ê³µê³  ë§¤ì¹­\\\\nì œê³µë°›ì€ ì´ë ¥ì„œë¥¼ RAGë¥¼ í™œìš©í•´ ê° íšŒì‚¬ì˜ ì±„ìš© ê³µê³ ì™€ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì í•©í•œ íšŒì‚¬ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\\\\nê¸°ì—… ë§ì¶¤í˜• ì´ë ¥ì„œ í”¼ë“œë°±\\\\nìœ ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ í‰ê°€-ê°œì„ -ì¬í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê±°ì¹˜ë©° ì´ë ¥ì„œë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.\\\\nMulti-turnì„ í™œìš©í•´ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìœ ì €ì—ê²Œ ê°œì„ ì ì„ ì œê³µí•˜ë©°, ì´ë ¥ì„œì— ê³§ë°”ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤. (ê¸°ëŠ¥ ë°˜ì˜ ì˜ˆì •)\\\\níšŒì‚¬ ì •ë³´ ìš”ì•½ ì œê³µ\\\\nAI Agentë¥¼ í†µí•´ ì›í•˜ëŠ” íšŒì‚¬ ë˜ëŠ” ì¶”ì²œëœ JDì˜ íšŒì‚¬ì™€ ê´€ë ¨ëœ ìµœì‹  ì •ë³´ë¥¼ ìš”ì•½í•´ ì œê³µí•©ë‹ˆë‹¤.\\\\nì‚°ì—… ë° ë„ë©”ì¸, ê²½ìŸë ¥, ì£¼ìš” ì„œë¹„ìŠ¤, ì¸ì¬ìƒ, ì‚¬ë‚´ ë¬¸í™”, ì¶”ì§„ ì¤‘ì¸ í”„ë¡œì íŠ¸ ë° ì´ë‹ˆì…”í‹°ë¸Œ ë“±ì— ëŒ€í•œ ë©”íƒ€ ë°ì´í„°ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\\\\nì´ë ¥ì„œ ì í•©ì„± í‰ê°€ ë° êµ¬ì²´ì  ê°œì„ ì  ì œì•ˆ\\\\nì´ë ¥ì„œ ê²€í†  ì‹œìŠ¤í…œì€ íŠ¹ì • JDì— ë§ì¶° ì´ë ¥ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì›ì ì¶”ì  ì‹œìŠ¤í…œ(ATS)ì´ ì´ë¥¼ ì–´ë–»ê²Œ í‰ê°€í• ì§€ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\\\\në˜í•œ, ê° ì´ë ¥ì„œë¥¼ í•´ë‹¹ ì§ë¬´ì— ìµœì í™”í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸í•œ í”¼ë“œë°±ê³¼ ê°œì„  ì œì•ˆë„ ì œê³µí•©ë‹ˆë‹¤. ì´ ë¶„ì„ê¸°ëŠ” ê° ì§ë¬´ ì„¤ëª…ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬, ì´ë ¥ì„œê°€ í•´ë‹¹ ì§ë¬´ì— ì–¼ë§ˆë‚˜ ì˜ ë¶€í•©í•˜ëŠ”ì§€ ê°€ì¥ ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\\\\nì‹œìŠ¤í…œ êµ¬ì¡°ë„\\\\nì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°ë„\\\\nì±„ìš© ê³µê³  ì¶”ì²œ RAG êµ¬ì¡°ë„\\\\nì´ë ¥ì„œ ìˆ˜ì • ì œì•ˆ ë° íšŒì‚¬ ìš”ì•½ Agent êµ¬ì¡°ë„\\\\nê²€í†  LLM íŒŒì´í”„ë¼ì¸\\\\nì‹œìŠ¤í…œ êµ¬ì„± ê¸°ìˆ \\\\ní¬ë¡¤ë§ API\\\\n: ì—¬ëŸ¬ íšŒì‚¬ JD ë°ì´í„° í™•ë³´ ë° ë§ˆê°ëœ ì±„ìš© ê³µê³  ì‚­ì œ\\\\nPDF íŒŒì‹±\\\\n: Upstage Parserë¥¼ ì´ìš©í•œ ë‹¤ì–‘í•œ í˜•íƒœì˜ ì´ë ¥ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\\\\nVector DB\\\\n: Pineconeì— Job description(JD) í…ìŠ¤íŠ¸ Chunk ì €ì¥ ë° JD Retrieval\\\\nAI Agent\\\\nsupervisor agentë¥¼ ë‘ì–´ ë©€í‹° agentë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\\\\níšŒì‚¬ ë©”íƒ€ ë°ì´í„° ì •ë³´ ìˆ˜ì§‘ ë° ìš”ì•½ (Tavily Search MCP ì‚¬ìš©)\\\\nìˆ˜ì • ì œì•ˆ agent (ìœ ì €ì˜ ì›¹ í¬íŠ¸í´ë¦¬ì˜¤ íƒìƒ‰ MCP)\\\\nLLM Model\\\\nì´ë ¥ì„œ ì í•©ì„± í‰ê°€\\\\nê¸°ìˆ  ìŠ¤íƒ\\\\nLanguage: Python, Typescript\\\\nLLM Framework\\\\nLangChain\\\\nLanggraph\\\\nOpenAI (text-embedding-3-small, gpt-4o, gpt-4o-mini, gpt-4.1-nano)\\\\nBackend\\\\nWeb Framework: FastAPI\\\\nVector Database: Pinecone\\\\nMonitoring: Langfuse\\\\nì„œë¹„ìŠ¤ ë°ëª¨ í™”ë©´\\\\nì‹œì—° ì˜ìƒ\\\\nì´ë ¥ì„œ ê¸°ë°˜ JD ì¶”ì²œ ê¸°ëŠ¥ (Phase 1)\\\\nì›¹ í™”ë©´\\\\ní™˜ê²½ ì„¸íŒ…\\\\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—\\\\n.env.example\\\\níŒŒì¼ì„ ì°¸ê³ í•˜ì—¬\\\\n.env\\\\níŒŒì¼ì„ ìƒì„±í•˜ê³  API í‚¤ ë° í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\\\\nFRONTEND_CORS_ORIGIN\\\\n: CORS ì •ì±…ì„ ì ìš©í•  í”„ë¡ íŠ¸ì—”ë“œ ì£¼ì†Œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. (e.g.,\\\\nhttp://localhost:3000\\\\n)\\\\nNODE_ENV\\\\n: í”„ë¡ íŠ¸ì—”ë“œ\\\\nNODE_ENV\\\\ní™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤. (\\\\ndevelopment\\\\në˜ëŠ”\\\\nproduction\\\\n)\\\\n(Optional) ë¡œì»¬ í™˜ê²½ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°, ê° ì„œë¹„ìŠ¤ì˜\\\\nrequirements.txt\\\\në˜ëŠ”\\\\npackage.json\\\\nì„ ì°¸ê³ í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\\\\nì‹œìŠ¤í…œ êµ¬ë™ ë°©ë²•\\\\nDockerë¥¼ ì´ìš©í•œ ì‹¤í–‰\\\\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ Docker Composeë¡œ ì „ì²´ ì„œë¹„ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\\\\ndocker compose up [-d] [--build]\\\\në¡œì»¬ í™˜ê²½ì—ì„œ ì§ì ‘ ì‹¤í–‰\\\\nAPI ì‹¤í–‰\\\\npython backend/main.py\\\\nì›¹ í™”ë©´ ì‹¤í–‰\\\\nnpm --prefix frontend run dev\\\\nAPI í˜¸ì¶œ ì˜ˆì‹œ (api_test.py ì°¸ì¡°)\\\\nimport requests\\\\n\\\\n# POST ìš”ì²­ í•¨ìˆ˜\\\\ndef send_post_request(resume_path):\\\\n    url = \\\\\"http://localhost:8000/matching\\\\\"  # ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€ê²½í•˜ì„¸ìš”.\\\\n    data = {\\\\\"resume_path\\\\\": resume_path}\\\\n\\\\n    try:\\\\n        response = requests.post(url, json=data)\\\\n        response.raise_for_status()  # ìƒíƒœ ì½”ë“œê°€ 200ë²ˆëŒ€ê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ\\\\n        print(\\\\\"POST ìš”ì²­ ì„±ê³µ:\\\\\", response.json())\\\\n    except requests.exceptions.RequestException as e:\\\\n        print(\\\\\"POST ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\\\\\", e)\\\\n\\\\n\\\\n# í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œ\\\\nsend_post_request(\\\\\"data/joannadrummond-cv.pdf\\\\\")\\\\nğŸ§­ Branch Strategy\\\\nì´ í”„ë¡œì íŠ¸ëŠ” ì•ˆì •ì ì¸ ë°°í¬ì™€ ìœ ì—°í•œ ê°œë°œì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ Git ë¸Œëœì¹˜ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\\\\në¸Œëœì¹˜\\\\nì—­í• \\\\në¹„ê³ \\\\nmain\\\\nìš´ì˜(Production)\\\\nì‹¤ì œ ë°°í¬ëœ ì•ˆì • ë²„ì „\\\\ndev\\\\nê°œë°œ(Development)\\\\nê¸°ëŠ¥ í†µí•© ë° í…ŒìŠ¤íŠ¸\\\\nrelease/x.y.z\\\\në°°í¬ ì¤€ë¹„(Release Candidate)\\\\nQA ë° ì•ˆì •í™” ë‹¨ê³„\\\\nfeature/*\\\\nê¸°ëŠ¥ ê°œë°œ(Feature)\\\\nìƒˆ ê¸°ëŠ¥ ë‹¨ìœ„ ê°œë°œ\\\\nhotfix/*\\\\nê¸´ê¸‰ ìˆ˜ì •(Hotfix)\\\\nìš´ì˜ ì¤‘ ë¹ ë¥¸ ë²„ê·¸ ìˆ˜ì •\\\\nëª¨ë“  ì½”ë“œëŠ” Pull Request(PR)ë¥¼ í†µí•´ì„œë§Œ ë³‘í•©ë˜ë©°, ì§ì ‘ pushëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\\\\nìì„¸í•œ ê°œë°œ ë° ë¸Œëœì¹˜ ìš´ì˜ ê°€ì´ë“œëŠ”\\\\nCONTRIBUTING.md\\\\nì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\", \"languages\": {}, \"about\": {\"releases\": \"Releases\", \"contributors\": \"Contributors\\\\n      6\"}}', name='scrape_repo_main_page', id='8faa2753-4408-4427-948d-a0999aedab54', tool_call_id='call_gn1FhSHsIkunzb8BvmbJPT8O'), AIMessage(content='**JobPT ë ˆí¬ì§€í† ë¦¬ ìš”ì•½**\\n\\n**ë ˆí¬ì§€í† ë¦¬ ì´ë¦„:** JobPT  \\n**ì†Œìœ ì:** Pseudo-Lab  \\n**ìŠ¤íƒ€ ìˆ˜:** 32  \\n**í¬í¬ ìˆ˜:** 5  \\n**ì„¤ëª…:** êµ¬ì§ìì˜ ì´ë ¥ì„œì™€ ì±„ìš© ê³µê³ ë¥¼ ë§¤ì¹­í•´ì£¼ê³ , ì±„ìš© ê³µê³ ì— ë§ëŠ” ì´ë ¥ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì£¼ëŠ” ì±„ìš©ê³µê³  ë§¤ì¹­ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\\n\\n### ì„œë¹„ìŠ¤ ê°œìš”\\ní˜„ëŒ€ ì‚¬íšŒì—ì„œ ì·¨ì—… ì¤€ë¹„ëŠ” ë°˜ë³µì ì´ê³  ì‹œê°„ ì†Œëª¨ì ì¸ ê³¼ì •ìœ¼ë¡œ, êµ¬ì§ìê°€ ìì‹ ì˜ ì—­ëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì–´í•„í•˜ì§€ ëª»í•´ ê¸°íšŒë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ JobPTëŠ” LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ì·¨ì—… ì§€ì› ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì±„ìš© ê³µê³  ì¶”ì²œ:** ì œê³µëœ ì´ë ¥ì„œë¥¼ RAG(íšŒê·€ì  ì–´í…ì…˜ ê·¸ë˜í”„)ë¥¼ í™œìš©í•˜ì—¬ ê° íšŒì‚¬ì˜ ì±„ìš© ê³µê³ ì™€ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì í•©í•œ íšŒì‚¬ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\\n2. **ì´ë ¥ì„œ í”¼ë“œë°±:** ìœ ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ì´ë ¥ì„œë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n3. **íšŒì‚¬ ì •ë³´ ìš”ì•½:** AI Agentë¥¼ í†µí•´ ì›í•˜ëŠ” íšŒì‚¬ ë˜ëŠ” ì¶”ì²œëœ JD(ì§ë¬´ ì„¤ëª…)ì˜ ìµœì‹  ì •ë³´ë¥¼ ìš”ì•½í•´ ì œê³µí•©ë‹ˆë‹¤.\\n4. **ì´ë ¥ì„œ ì í•©ì„± í‰ê°€:** ì´ë ¥ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì›ì ì¶”ì  ì‹œìŠ¤í…œ(ATS)ì´ ì´ë¥¼ ì–´ë–»ê²Œ í‰ê°€í• ì§€ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\\n\\n### ê¸°ìˆ  ìŠ¤íƒ\\n- **í”„ë¡œê·¸ë˜ë° ì–¸ì–´:** Python, Typescript\\n- **LLM í”„ë ˆì„ì›Œí¬:** LangChain, Langgraph, OpenAI\\n- **ë°±ì—”ë“œ:** FastAPI\\n- **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤:** Pinecone\\n- **ëª¨ë‹ˆí„°ë§:** Langfuse\\n\\n### ì‹œìŠ¤í…œ êµ¬ì„±\\n- **í¬ë¡¤ë§ API:** ì—¬ëŸ¬ íšŒì‚¬ JD ë°ì´í„° í™•ë³´ ë° ë§ˆê°ëœ ì±„ìš© ê³µê³  ì‚­ì œ\\n- **PDF íŒŒì‹±:** ë‹¤ì–‘í•œ í˜•íƒœì˜ ì´ë ¥ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\\n- **AI Agent:** ë©€í‹° ì—ì´ì „íŠ¸ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\\n\\n### ì‹¤í–‰ ë°©ë²•\\n- Dockerë¥¼ ì´ìš©í•œ ì‹¤í–‰ ë° ë¡œì»¬ í™˜ê²½ì—ì„œì˜ ì§ì ‘ ì‹¤í–‰ ë°©ë²•ì´ ì œê³µë©ë‹ˆë‹¤. API í˜¸ì¶œ ì˜ˆì‹œë„ í¬í•¨ë˜ì–´ ìˆì–´ ì‚¬ìš©ìê°€ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\n### ë¸Œëœì¹˜ ì „ëµ\\n- **main:** ìš´ì˜(Production)\\n- **dev:** ê°œë°œ(Development)\\n- **feature/*:** ê¸°ëŠ¥ ê°œë°œ(Feature)\\n- **hotfix/*:** ê¸´ê¸‰ ìˆ˜ì •(Hotfix)\\n\\nì´ ë ˆí¬ì§€í† ë¦¬ëŠ” êµ¬ì§ìì—ê²Œ ë§ì¶¤í˜• ì±„ìš© ê³µê³ ë¥¼ ì¶”ì²œí•˜ê³  ì´ë ¥ì„œë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ì·¨ì—… ì¤€ë¹„ ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 589, 'prompt_tokens': 2050, 'total_tokens': 2639, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CXnGDEVRt6CvRGjMBdYb2MX4xgAkX', 'finish_reason': 'stop', 'logprobs': None}, id='run--247a25d3-4faf-4997-b944-287f07fb6ce1-0', usage_metadata={'input_tokens': 2050, 'output_tokens': 589, 'total_tokens': 2639, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "result = await github_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507805d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**JobPT ë ˆí¬ì§€í† ë¦¬ ìš”ì•½**\\n\\n**ë ˆí¬ì§€í† ë¦¬ ì´ë¦„:** JobPT  \\n**ì†Œìœ ì:** Pseudo-Lab  \\n**ìŠ¤íƒ€ ìˆ˜:** 32  \\n**í¬í¬ ìˆ˜:** 5  \\n**ì„¤ëª…:** êµ¬ì§ìì˜ ì´ë ¥ì„œì™€ ì±„ìš© ê³µê³ ë¥¼ ë§¤ì¹­í•´ì£¼ê³ , ì±„ìš© ê³µê³ ì— ë§ëŠ” ì´ë ¥ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì£¼ëŠ” ì±„ìš©ê³µê³  ë§¤ì¹­ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\\n\\n### ì„œë¹„ìŠ¤ ê°œìš”\\ní˜„ëŒ€ ì‚¬íšŒì—ì„œ ì·¨ì—… ì¤€ë¹„ëŠ” ë°˜ë³µì ì´ê³  ì‹œê°„ ì†Œëª¨ì ì¸ ê³¼ì •ìœ¼ë¡œ, êµ¬ì§ìê°€ ìì‹ ì˜ ì—­ëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì–´í•„í•˜ì§€ ëª»í•´ ê¸°íšŒë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ JobPTëŠ” LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸) ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ì·¨ì—… ì§€ì› ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì±„ìš© ê³µê³  ì¶”ì²œ:** ì œê³µëœ ì´ë ¥ì„œë¥¼ RAG(íšŒê·€ì  ì–´í…ì…˜ ê·¸ë˜í”„)ë¥¼ í™œìš©í•˜ì—¬ ê° íšŒì‚¬ì˜ ì±„ìš© ê³µê³ ì™€ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì í•©í•œ íšŒì‚¬ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\\n2. **ì´ë ¥ì„œ í”¼ë“œë°±:** ìœ ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ì´ë ¥ì„œë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n3. **íšŒì‚¬ ì •ë³´ ìš”ì•½:** AI Agentë¥¼ í†µí•´ ì›í•˜ëŠ” íšŒì‚¬ ë˜ëŠ” ì¶”ì²œëœ JD(ì§ë¬´ ì„¤ëª…)ì˜ ìµœì‹  ì •ë³´ë¥¼ ìš”ì•½í•´ ì œê³µí•©ë‹ˆë‹¤.\\n4. **ì´ë ¥ì„œ ì í•©ì„± í‰ê°€:** ì´ë ¥ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì›ì ì¶”ì  ì‹œìŠ¤í…œ(ATS)ì´ ì´ë¥¼ ì–´ë–»ê²Œ í‰ê°€í• ì§€ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\\n\\n### ê¸°ìˆ  ìŠ¤íƒ\\n- **í”„ë¡œê·¸ë˜ë° ì–¸ì–´:** Python, Typescript\\n- **LLM í”„ë ˆì„ì›Œí¬:** LangChain, Langgraph, OpenAI\\n- **ë°±ì—”ë“œ:** FastAPI\\n- **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤:** Pinecone\\n- **ëª¨ë‹ˆí„°ë§:** Langfuse\\n\\n### ì‹œìŠ¤í…œ êµ¬ì„±\\n- **í¬ë¡¤ë§ API:** ì—¬ëŸ¬ íšŒì‚¬ JD ë°ì´í„° í™•ë³´ ë° ë§ˆê°ëœ ì±„ìš© ê³µê³  ì‚­ì œ\\n- **PDF íŒŒì‹±:** ë‹¤ì–‘í•œ í˜•íƒœì˜ ì´ë ¥ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\\n- **AI Agent:** ë©€í‹° ì—ì´ì „íŠ¸ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\\n\\n### ì‹¤í–‰ ë°©ë²•\\n- Dockerë¥¼ ì´ìš©í•œ ì‹¤í–‰ ë° ë¡œì»¬ í™˜ê²½ì—ì„œì˜ ì§ì ‘ ì‹¤í–‰ ë°©ë²•ì´ ì œê³µë©ë‹ˆë‹¤. API í˜¸ì¶œ ì˜ˆì‹œë„ í¬í•¨ë˜ì–´ ìˆì–´ ì‚¬ìš©ìê°€ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\\n\\n### ë¸Œëœì¹˜ ì „ëµ\\n- **main:** ìš´ì˜(Production)\\n- **dev:** ê°œë°œ(Development)\\n- **feature/*:** ê¸°ëŠ¥ ê°œë°œ(Feature)\\n- **hotfix/*:** ê¸´ê¸‰ ìˆ˜ì •(Hotfix)\\n\\nì´ ë ˆí¬ì§€í† ë¦¬ëŠ” êµ¬ì§ìì—ê²Œ ë§ì¶¤í˜• ì±„ìš© ê³µê³ ë¥¼ ì¶”ì²œí•˜ê³  ì´ë ¥ì„œë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ì·¨ì—… ì¤€ë¹„ ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['company_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06285c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
