#!/usr/bin/env python3
"""
JD URL í¬ë¡¤ëŸ¬ - ê°„ë‹¨í•œ CLI ë„êµ¬

ì‚¬ìš©ë²•:
    python3 backend/util/demo_crawl_url.py <URL>
    python3 backend/util/demo_crawl_url.py https://www.saramin.co.kr/zf_user/jobs/relay/view?isMypage=no&rec_idx=52554116&recommend_ids=eJxtz8sRwjAMBNBquEva1e9MIem%2FCwgTHJnh%2BCyPduVm3d55lOYjn25OptTRYj88%2BHmgaUSs75LRxCIQabbIUOv4w2sZioFcYTfXXLx7douK2e3itxsFNuJOYqPWvUyl590w840OTrJ8m%2Fo4880tCITuNeTkCwNGT3g%3D&view_type=etc&gz=1&t_ref_content=banner&t_ref=view_delete&relayNonce=7f8030cbbed0ae057f36&immediately_apply_layer_open=n
    
    # íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ë©´:
    python3 backend/util/demo_crawl_url.py <URL> --save
"""

import sys
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.util.jd_crawler import crawl_jd_from_url


def save_to_file(url, result):
    """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = Path.cwd() / "scraped_jd"
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    site_name = result.get('site', 'unknown').replace(' ', '_')
    filename = f"{site_name}_{timestamp}.txt"
    filepath = output_dir / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"URL: {url}\n")
        f.write(f"ì‚¬ì´íŠ¸: {result['site']}\n")
        f.write(f"ì„±ê³µ ì—¬ë¶€: {result['success']}\n")
        f.write(f"ì—ëŸ¬: {result['error']}\n")
        f.write(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])} ì\n")
        f.write("=" * 80 + "\n\n")
        f.write(result['text'])
    
    return filepath


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python3 backend/util/demo_crawl_url.py <URL> [--save]")
        print("\nì˜ˆì‹œ:")
        print("  python3 backend/util/demo_crawl_url.py https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx=12345")
        print("  python3 backend/util/demo_crawl_url.py <URL> --save  # íŒŒì¼ë¡œ ì €ì¥")
        sys.exit(1)
    
    url = sys.argv[1]
    save_file = '--save' in sys.argv
    
    print("\n" + "=" * 80)
    print(f"  JD URL í¬ë¡¤ë§")
    print("=" * 80)
    print(f"\nğŸ“Œ URL: {url}\n")
    
    try:
        result = crawl_jd_from_url(url)
        
        print("â”€" * 80)
        if result['success']:
            print(f"âœ… ì„±ê³µ")
            print(f"ğŸ¢ ì‚¬ì´íŠ¸: {result['site']}")
            print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])} ì")
            
            if save_file:
                filepath = save_to_file(url, result)
                print(f"ğŸ’¾ íŒŒì¼ ì €ì¥: {filepath}")
                print("\n" + "â”€" * 80)
                print("ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì):")
                print("â”€" * 80)
                print(result['text'][:500])
                if len(result['text']) > 500:
                    print(f"\n... (ì´ {len(result['text'])}ì, ì „ì²´ëŠ” íŒŒì¼ ì°¸ì¡°)")
                print("â”€" * 80)
            else:
                print("\n" + "â”€" * 80)
                print("ğŸ“„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:")
                print("â”€" * 80)
                print(result['text'])
                print("â”€" * 80)
                print(f"\nğŸ’¡ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ë©´: --save ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”")
        else:
            print(f"âŒ ì‹¤íŒ¨")
            print(f"ğŸ¢ ì‚¬ì´íŠ¸: {result['site']}")
            print(f"âš ï¸  ì—ëŸ¬: {result['error']}")
            print("â”€" * 80)
    
    except Exception as e:
        print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
