from langchain_upstage import ChatUpstage
from configs import *
import numpy as np
from collections import defaultdict

llm = ChatUpstage(model=RAG_MODEL, api_key=UPSTAGE_API_KEY)
search_dict = defaultdict(list)

def make_rank(results,k, full=False):
    ## queryì™€ dbë¥¼ ë„£ìœ¼ë©´ idì˜ listë¥¼ ë¦¬í„´
    search_range = min(k, len(results))
    scores = np.empty(search_range, dtype=object)
    for i in range(search_range):
        scores[i] = results[i].metadata['id']
        search_dict[results[i].metadata['id']].append(results[i])
    return scores

def rrf(multi_scores, k=1):        #n*10ê°œì˜ ì…ë ¥, idë¡œ ë“¤ì–´ì˜´
    score = 0.0
    score_dict = defaultdict(int)
    for scores in multi_scores:
        for rank, id in enumerate(scores):
            score = 1.0 / ( k + rank+1)       #indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1
            score_dict[id]+=score
    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
    return score_dict



async def search_jd(retriever, lexical_retriever, resume):
    """
    ì‚¬ìš©ìì˜ ì´ë ¥ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° DBì—ì„œ ì±„ìš©ê³µê³ ë¥¼ ê²€ìƒ‰í•˜ê³ 
    LLMì„ ì´ìš©í•´ CV, JD ë¦¬ë·°ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    Args:
        retriever: semantic retriever
        lexical_retriever: lexical retriever
        resume: ì‚¬ìš©ìì˜ ì´ë ¥ì„œ
    Returns:
        answer: LLMì„ í†µí•œ CV, JD ë¦¬ë·°
        top_job_description: ì²« ë²ˆì§¸ ë¬¸ì„œ(top-similarity)ì˜ ì±„ìš©ê³µê³  ì „ë¬¸
        top_job_url: ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì±„ìš©ê³µê³  URL
        top_company_name: ì²« ë²ˆì§¸ ë¬¸ì„œì˜ íšŒì‚¬ ì´ë¦„
    """
    print("\n=== Generation í•¨ìˆ˜ ì‹œì‘ ===")
    print("ì…ë ¥ëœ resume:", resume[:100], "...")  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¼ë¶€ë§Œ ì¶œë ¥
    job_descriptions = retriever.invoke(resume)

    ### í•œêµ­ì–´ BM25 retrieval ì¶”ê°€ì‹œ í™œìš©
    if lexical_retriever:
        lexical_job_descriptions = lexical_retriever.invoke(resume)
        sem_rank = make_rank(job_descriptions, k=10)
        lex_rank = make_rank(lexical_job_descriptions, k=10)
        job_descriptions = search_dict[rrf([sem_rank, lex_rank], k=1.2)[0][0]]

    # Retriever ì‹¤í–‰
    print("\n=== Retriever ì‹¤í–‰ ===")
    # print("job_descriptions:", job_descriptions)      # retrievalëœ ëª¨ë“  ê²°ê³¼ ì¶œë ¥



    # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if not job_descriptions:
        print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        return "No matches found", "", "", ""


    # Metadata ì ‘ê·¼(ì±„ìš©ê³µê³  ì „ë¬¸ í¬í•¨)
    try:
        top_job_description = job_descriptions[0].metadata["description"]
        top_job_url = job_descriptions[0].metadata["job_url"]
        top_company_name = job_descriptions[0].metadata["company"]
        print("==================================================")
        print("\nì²« ë²ˆì§¸ ë¬¸ì„œ ì „ë¬¸ ë° ë©”íƒ€ë°ì´í„° í™•ì¸:")
        print("JD ì „ë¬¸:", top_job_description)
        print("==================================================")
        print("job_url:", top_job_url)
        print("company:", top_company_name)
    except Exception as e:
        print("ë©”íƒ€ë°ì´í„° ì ‘ê·¼ ì¤‘ ì—ëŸ¬:", str(e))
        return "Error accessing metadata", "", "", ""



    return top_job_description, top_job_url, top_company_name


async def search_jd_summary(retriever, lexical_retriever, resume, pinecone_index=None):
    from get_similarity.utils.segmenter import HierarchicalSegmenter
    from get_similarity.utils.matcher import DenseMatcher
    
    print("\n=== Summary ê²€ìƒ‰ í•¨ìˆ˜ ì‹œì‘ ===")
    print("ì…ë ¥ëœ resume:", resume[:100], "...")  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¼ë¶€ë§Œ ì¶œë ¥

    # 1. ë¬¸ì„œ ê²€ìƒ‰ (Candidate Selection)
    # ê¸°ì¡´ invoke(resume)ëŠ” 'ë¬¸ì„œ' ë‹¨ìœ„ ìœ ì‚¬ë„ë¡œ kê°œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # í•˜ì§€ë§Œ ìš°ë¦¬ì˜ ëª©í‘œëŠ” 'JD id' í›„ë³´êµ°ì„ ë½‘ëŠ” ê²ƒì…ë‹ˆë‹¤.
    # ë”°ë¼ì„œ ë„‰ë„‰í•˜ê²Œ k=30~50ê°œë¥¼ ê°€ì ¸ì™€ì„œ unique job_idë¥¼ ì¶”ë¦½ë‹ˆë‹¤.
    
    # retriever search_kwargs ì—…ë°ì´íŠ¸ (ì ì‹œ í›„ë³´êµ° íƒìƒ‰ìš©ìœ¼ë¡œ í™•ì¥)
    original_k = retriever.search_kwargs.get("k", 10)
    retriever.search_kwargs["k"] = 50
    
    candidates = retriever.invoke(resume)
    
    # 2. í›„ë³´êµ° Job ID ì¶”ì¶œ
    candidate_job_ids = set()
    job_to_metadata = {}
    
    candidate_job_ids = set()
    job_to_metadata = {}
    
    if candidates:
        print(f"[DEBUG] ì²« ë²ˆì§¸ ë¬¸ì„œ Metadata ì˜ˆì‹œ: {candidates[0].metadata}")

    for doc in candidates:
        job_id = None
        # 1. doc.id íŒŒì‹± (wd_1234__c0001)
        did = getattr(doc, "id", "")
        if did and "__" in did:
            job_id = did.split("__")[0]
            
        # 2. metadata í™•ì¸
        if not job_id:
             job_id = doc.metadata.get("job_id")
        
        # 3. URL í™•ì¸
        if not job_id:
             url = doc.metadata.get("job_url") or doc.metadata.get("url")
             if url:
                 job_id = str(url).rstrip("/").split("/")[-1]

        if job_id:
            candidate_job_ids.add(job_id)
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            # ì¤‘ìš”: doc.metadataì— íšŒì‚¬ëª… ë“±ì´ ìˆì–´ì•¼ í•¨.
            if job_id not in job_to_metadata:
                 job_to_metadata[job_id] = doc.metadata

    print(f"í›„ë³´êµ° ì¶”ì¶œ ì™„ë£Œ: {len(candidate_job_ids)}ê°œì˜ ê³ ìœ  JD ë°œê²¬")

    # Pinecone Indexê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§(search_dict ì‚¬ìš©)ìœ¼ë¡œ fallback
    if not pinecone_index:
        print("[WARN] Pinecone Index ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë‹¨ìˆœ ê²€ìƒ‰ ë¡œì§ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        retriever.search_kwargs["k"] = original_k # ë³µêµ¬
        job_descriptions = candidates[:10] # ìƒìœ„ 10ê°œë§Œ
        # ... (ê¸°ì¡´ RRF ë¡œì§ ìƒëµ, í•„ìš”í•œ ê²½ìš° ì¶”ê°€) ...
    else:
        # 3. Dense Multi-aspect Re-ranking ìˆ˜í–‰
        print(">>> Dense Multi-aspect Re-ranking ì‹œì‘")
        
        # (A) CV Segmentation & Embedding
        # EmbedderëŠ” retrieverê°€ ê°€ì§€ê³  ìˆëŠ” ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ìƒˆë¡œ ì„ ì–¸
        # ì—¬ê¸°ì„œëŠ” retriever.embeddings ê°ì²´ê°€ ìˆë‹¤ê³  ê°€ì • (LangChain í‘œì¤€)
        segmenter = HierarchicalSegmenter(min_chunk_length=100, max_chunk_length=300)
        cv_chunks = segmenter.segment(resume) 
        if not cv_chunks:
            cv_chunks = segmenter._segment_plaintext(resume)
            
        print(f"CV ì²­í¬ ë¶„í• : {len(cv_chunks)}ê°œ")
        
        # CV ì²­í¬ ë²¡í„°í™”
        # retrieverê°€ embedding_functionì„ ê°€ì§€ê³  ìˆìŒ
        embedding_model = retriever.tags[0] if hasattr(retriever, "tags") else None
        # LangChain Retriever êµ¬ì¡°ìƒ embedding_function ì ‘ê·¼ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ.
        # ê°€ì¥ í™•ì‹¤í•œ ê±´ ì „ë‹¬ë°›ì€ retriever ê°ì²´ ë‚´ë¶€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„.
        # ë§Œì•½ ì ‘ê·¼ì´ ì–´ë µë‹¤ë©´ main.pyì—ì„œ emb_modelì„ ë„˜ê²¨ë°›ëŠ” ê²Œ ì¢‹ìœ¼ë‚˜,
        # ì—¬ê¸°ì„œëŠ” retriever.vectorstore.embedding_function ë“±ì„ ì‹œë„í•˜ê±°ë‚˜ 
        # OpenAIEmbeddingsì„ ìƒˆë¡œ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë…¸ë ¥í•¨.
        
        # ì ‘ê·¼ë²•: retriever -> vectorstore -> embeddings
        emb_fn = None
        if hasattr(retriever, "vectorstore"):
             emb_fn = retriever.vectorstore.embeddings
        
        if not emb_fn:
            # Fallback: ìƒˆë¡œ ìƒì„± (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ì•ˆì „)
            from langchain_openai import OpenAIEmbeddings
            emb_fn = OpenAIEmbeddings()

        cv_vectors = emb_fn.embed_documents(cv_chunks)
        cv_vectors_np = [np.array(v) for v in cv_vectors]

        # (B) JD Full Vectors Fetching
        # [Full Scan Mode Implementation]
        # ë…¸íŠ¸ë¶ ë¡œì§ ë³µì œ: DB ì „ì²´ ë²¡í„°ë¥¼ ê°€ì ¸ì™€ì„œ Scoring ìˆ˜í–‰
        
        # 2. JD Full Scan (Pineconeì—ì„œ ëª¨ë“  ë²¡í„° ê°€ì ¸ì˜¤ê¸°)
        TOTAL_VECTORS_TO_FETCH = 2000 # temp ì¸ë±ìŠ¤ ì „ì²´ í¬ê¸° ì»¤ë²„ (ì•½ 2000ê°œ)
        print(f">>> 2. JD ì „ì²´ ë°ì´í„° ë¡œë“œ (Full Scan Mode, Target: {TOTAL_VECTORS_TO_FETCH} vectors)")
        
        # ì´ì „ ë‹¨ê³„ candidates ë£¨í”„ì—ì„œ ìˆ˜ì§‘í•œ ë©”íƒ€ë°ì´í„°ëŠ” ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìˆ˜ì§‘ (Full Scanì´ë¯€ë¡œ)
        job_embeddings_map = defaultdict(list)
        job_to_metadata = {}
        
        if pinecone_index:
            try:
                # CV ì²« ì²­í¬ ë²¡í„°ë¥¼ Queryë¡œ ì‚¬ìš© (ì–´ì°¨í”¼ kê°€ ë§¤ìš° ì»¤ì„œ ë‹¤ ë”¸ë ¤ì˜´)
                # ë§Œì•½ ì°¨ì›ì´ 4096ì´ ì•„ë‹ˆë¼ë©´ ì„ë² ë”© ëª¨ë¸ í™•ì¸ í•„ìš”.
                query_vec = cv_vectors[0]
                
                # include_metadata=True, include_values=True í•„ìˆ˜
                resp = pinecone_index.query(
                    vector=query_vec, 
                    top_k=TOTAL_VECTORS_TO_FETCH, 
                    include_metadata=True, 
                    include_values=True,
                    namespace=""
                )
                
                matches = resp.get("matches", [])
                print(f"Pinecone Full Query ì™„ë£Œ: {len(matches)}ê°œ ì²­í¬ í™•ë³´")
                
                for m in matches:
                    # m is ScoredVector (id, score, values, metadata)
                    vid = m["id"]
                    vals = m["values"]
                    meta = m.get("metadata", {})
                    
                    # Job ID ì¶”ì¶œ
                    job_id = None
                    if "__" in vid:
                        job_id = vid.split("__")[0]
                    # ë©”íƒ€ë°ì´í„°ì— job_idê°€ ìˆëŠ” ê²½ìš°
                    elif meta and "job_id" in meta:
                        job_id = str(meta["job_id"])
                    
                    if job_id:
                        # ë©”íƒ€ë°ì´í„° ì €ì¥ (íšŒì‚¬ëª… ë“±)
                        if job_id not in job_to_metadata:
                            job_to_metadata[job_id] = meta
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (Coverageìš©)
                        text_content = str(meta.get("text", "") or meta.get("chunk_text", "") or meta.get("context", ""))

                        job_embeddings_map[job_id].append({
                            "text": text_content,
                            "values": np.array(vals)
                        })
                        
            except Exception as e:
                print(f"Pinecone Full Query Error: {e}")
                import traceback
                traceback.print_exc()
                
        else:
            print("[ERROR] Pinecone Index Required for Full Scan")

        print(f"JDs ì¬ì¡°ë¦½ ì™„ë£Œ: {len(job_embeddings_map)}ê°œ Job (ìœ íš¨ ë²¡í„° ë³´ìœ )")

        # 3. Scoring (Parallel & JIT Optimized)
        print(">>> 3. Dense Multi-aspect Scoring (Similarity Only)")
        matcher = DenseMatcher(num_workers=4)
        
        scored_jobs = matcher.compute_batch_parallel(cv_vectors_np, job_embeddings_map)
        
        # ë©”íƒ€ë°ì´í„° ë³‘í•©
        for job in scored_jobs:
            jid = job["job_id"]
            job["metadata"] = job_to_metadata.get(jid, {})

        # ì •ë ¬
        scored_jobs.sort(key=lambda x: x["final_score"], reverse=True)
        top_jobs = scored_jobs[:4]
        
        # ê²°ê³¼ í¬ë§·íŒ…
        top_job_summaries = []
        top_job_urls = []
        top_company_names = []
        
        print("\n>>> ê²€ìƒ‰ ê²°ê³¼ í™•ì¸")
        print("-" * 60)

        for job in top_jobs:
            meta = job["metadata"]
            url = meta.get("job_url") or meta.get("url") or ""
            company = meta.get("company_name") or meta.get("company") or ""
            summary = meta.get("summary") or meta.get("text") or "ìš”ì•½ ì—†ìŒ"
            
            # ì¤‘ìš”: None ë°©ì§€
            if not company and job["job_id"]:
                 company = f"Job {job['job_id']}"
            
            # Coverage ì¶œë ¥ ì œê±°
            print(f"[Rank] Score: {job['final_score']:.4f} | {company}")
            print(f"ğŸ”— URL: {url}")
            print(f"ğŸ“ ìš”ì•½: {str(summary)[:100]}...")
            print("-" * 30)

            top_job_summaries.append(summary)
            top_job_urls.append(url)
            top_company_names.append(company)

        retriever.search_kwargs["k"] = original_k # ë³µêµ¬
        return top_job_summaries, top_job_urls, top_company_names
    
